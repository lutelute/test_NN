# Grokking 現象の可視化 - テクニカルノート

## 概要

このプロジェクトは、ニューラルネットワークにおける **Grokking現象**（突然の汎化）を可視化し、内部表現の変化を観察するためのツールです。

## Grokking現象とは

### 語源

**Grokking**（グロッキング）は、SF作家ロバート・A・ハインラインの小説『異星の客』(1961) に登場する火星語で、「深く直感的に理解する」という意味です。

機械学習の文脈では、「モデルが突然、本質的な理解に到達する」現象を指します。

### 定義

**Grokking** = 過学習（過剰適合）状態が長期間続いた後、突然汎化が起こる現象

Power et al. (2022) "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets" で初めて報告されました。

### 通常の学習 vs Grokking

**通常の機械学習:**
```
Epoch:     0 -----> 100 -----> 200
Train Acc: 0% ----> 80% -----> 95%
Test Acc:  0% ----> 75% -----> 90%
           ↑ 訓練とテストが同時に改善
```

**Grokking:**
```
Epoch:     0 -----> 100 -----> 500 -----> 600
Train Acc: 0% ----> 100% ----> 100% ----> 100%
Test Acc:  0% ---->  1% ---->   5% ----> 100%
                    ↑                    ↑
               過学習完了            突然の汎化!
```

### なぜ不思議なのか

従来の機械学習の常識:
1. **過学習 = 悪**: 訓練データを丸暗記すると汎化しない
2. **早期停止**: 過学習が始まったら学習を止めるべき
3. **汎化は同時**: 訓練精度と一緒にテスト精度も上がる

Grokkingはこれらの常識に反します:
1. **過学習の先に汎化がある**: 過学習状態を超えて学習を続けると汎化
2. **早期停止は逆効果**: 途中で止めたら汎化しない
3. **汎化は遅延する**: 訓練精度100%のずっと後にテスト精度が上がる

### Grokkingの発生条件

1. **小さなデータセット**: 全パターンの一部のみで訓練
2. **アルゴリズム的タスク**: 背後に数学的構造がある
3. **強い正則化**: 特に weight decay が重要
4. **十分な学習時間**: 通常の10〜100倍のエポック数

### 本質的に何が起きているのか

```
【記憶フェーズ】(Epoch 0-200)
- モデルは訓練データを「丸暗記」
- 各入力に対する出力を個別に記憶
- テストデータには対応できない（汎化なし）

【再構成フェーズ】(Epoch 200-500)
- Weight decay が記憶表現を徐々に「圧縮」
- より効率的な表現（フーリエ基底）に再構成
- 内部表現が根本的に変化

【汎化フェーズ】(Epoch 500+)
- フーリエ表現が完成
- 同じアルゴリズムで全ての入力に対応可能
- Test Acc が急上昇（Grokking!）
```

### 視覚化での観察ポイント

| 指標 | 記憶フェーズ | 汎化フェーズ |
|------|-------------|-------------|
| Train Acc | 100% | 100% |
| Test Acc | ~0% | ~100% |
| Circle値 | 0.1-0.3 | 0.9+ |
| 円環構造 | ランダム | きれいな円 |
| 相関行列 | 散らばり | 円形パターン |

## タスク: モジュラー加算（Modular Addition）

### モジュラー加算とは

**モジュラー演算（剰余演算）**は、ある数で割った余りを扱う演算です。

```
通常の加算:    5 + 3 = 8
モジュラー加算: (5 + 3) mod 7 = 1  （8を7で割った余り）
```

**時計の例:**
```
現在 10時、3時間後は？
(10 + 3) mod 12 = 1時

現在 10時、5時間後は？
(10 + 5) mod 12 = 3時
```

### なぜ「足し算」を学習させるのか

1. **シンプルだが非自明**:
   - 通常の足し算は線形（直線的）
   - モジュラー加算は周期的（循環的）→ 非線形性が必要

2. **数学的に解析可能**:
   - フーリエ変換で厳密に解析できる
   - モデルが「何を学んだか」を検証可能

3. **Grokkingが観察しやすい**:
   - 小さなデータセットで記憶 vs 汎化の境界が明確

### 本プロジェクトの設定

```
入力: (a, b) where a, b ∈ {0, 1, ..., 96}
出力: (a + b) mod 97

例:
(50, 30) → 80        (50 + 30 = 80 < 97)
(50, 60) → 13        (50 + 60 = 110, 110 mod 97 = 13)
(96, 1)  → 0         (96 + 1 = 97, 97 mod 97 = 0)
```

- **p = 97**（素数を使う理由: 周期構造が単純になる）
- **全ペア数**: 97 × 97 = 9,409 通り
- **訓練データ**: 30%（約2,800ペア）
- **テストデータ**: 70%（約6,600ペア）

### なぜ素数 p を使うのか

素数を使うと、モジュラー演算が**巡回群**を形成します：

```
p=5 の場合の足し算テーブル:
  + | 0 1 2 3 4
  --+-----------
  0 | 0 1 2 3 4
  1 | 1 2 3 4 0  ← 4+1=5≡0
  2 | 2 3 4 0 1
  3 | 3 4 0 1 2
  4 | 4 0 1 2 3
```

この構造がフーリエ表現と相性が良い（群のフーリエ解析）。

## フーリエ表現理論

Grokking後、モデルは内部的に**フーリエ表現**を学習します。

### 理論的背景

モジュラー加算を解くために、モデルは数値 n を以下のように埋め込みます：

```
e(n) ≈ [cos(2πkn/p), sin(2πkn/p)]  (周波数 k のフーリエ基底)
```

この表現を使うと、加算が以下のように計算できます：

```
e(a) + e(b) の角度 = 2πk(a+b)/p mod 2π
```

### 円環構造

フーリエ表現が学習されると、和の値 `s = (a+b) mod p` に対する内部表現は**円環上**に配置されます：

```
       s=24
      ·   ·  s=0
    ·       ·
  s=48       s=72
    ·       ·
      ·   ·
       s=96
```

## 可視化ダッシュボード

### 起動方法

```bash
source venv/bin/activate
streamlit run interactive_dashboard.py --server.port 8502
```

### 主要機能

#### 1. Epoch Slider タブ

学習過程をアニメーションで観察：

- **左パネル**: 円環構造（(a+b) mod p の表現）
- **右パネル**: 7×7 ニューロン相関行列
- **下パネル**: 学習曲線 + 現在位置

#### 2. 円環構造の検出

各エポックで以下を計算：

1. 全ての和値 s ∈ {0, ..., p-1} に対するpooled層出力を取得
2. cos(2πks/p) と sin(2πks/p) に最も相関する次元を検出
3. その2次元でプロット → 円環が形成されれば Fourier 表現を学習

**Circle値**（角度相関）:
- < 0.5: 円環未形成（記憶段階）
- \> 0.9: 円環形成（Fourier表現学習済み）

#### 3. ニューロン相関行列

上位7次元のニューロン出力の散布図グリッド：
- 対角: 各次元の分布
- 非対角: 次元間の相関

Grokking前後で構造が劇的に変化します。

## 実装詳細

### cos/sin ペア検出アルゴリズム

```python
for k in range(1, 20):
    cos_basis = np.cos(2 * np.pi * k * s_values / p)
    sin_basis = np.sin(2 * np.pi * k * s_values / p)

    # 各次元との相関を計算
    for d in range(d_model):
        cos_corr[d] = corrcoef(embeddings[:, d], cos_basis)
        sin_corr[d] = corrcoef(embeddings[:, d], sin_basis)

    # 最良のcos次元とsin次元を選択（異なる次元）
    best_cos_dim = argmax(|cos_corr|)
    best_sin_dim = argmax(|sin_corr|, exclude=best_cos_dim)
```

### データ効率化

大量のチェックポイントを扱うため：

1. **サンプリング**: 各和値に5サンプル（p×5 = 485点）
2. **平均化**: 同じ和値のサンプルを平均
3. **フレーム間引き**: 最大50フレームに制限

## チェックポイント

| ディレクトリ | 刻み | ファイル数 | 用途 |
|-------------|------|-----------|------|
| `checkpoints_demo` | 10 epoch | 500 | 概観 |
| `checkpoints_demo_2ep` | 2 epoch | 2500 | 詳細観察 |
| `checkpoints_demo_5ep` | 5 epoch | 1000 | バランス |

## 学習パラメータ

```python
p = 97              # 素数
d_model = 128       # 埋め込み次元
n_heads = 4         # アテンションヘッド
n_layers = 1        # Transformer層数
lr = 1e-3           # 学習率
weight_decay = 1.0  # 重み減衰（重要！）
train_ratio = 0.3   # 訓練データ比率
```

**Note**: `weight_decay=1.0` が Grokking に重要。小さいと汎化しない。

## 観察されるタイムライン

```
Epoch     0-50:   ランダム状態、Circle ≈ 0.1
Epoch   50-150:   訓練データ記憶、Train Acc → 100%
Epoch  150-200:   内部表現の再構成開始
Epoch  200-250:   Grokking! Test Acc 急上昇、Circle → 0.9+
Epoch  250+:      安定した Fourier 表現
```

## 各図の見方・解釈ガイド

### 1. 円環構造（左パネル）

```
    ● s=24
  ●     ● s=0
●         ●
  ●     ●
    ● s=72
```

**何を表しているか:**
- 各点は「和の値 s = (a+b) mod p」に対するモデルの内部表現
- p=97 なので 97個の点がプロットされる
- 色は HSV カラーマップで s=0 から s=96 まで連続的に変化

**解釈方法:**

| 状態 | 見た目 | 意味 |
|------|--------|------|
| 学習初期 | ランダムな点の散らばり | モデルは構造を学習していない |
| 記憶段階 | 不規則なクラスタ | 訓練データを丸暗記 |
| Grokking後 | きれいな円環 | フーリエ表現を学習 |

**Circle値の解釈:**
- 角度と理論値 2πs/p の相関係数
- 0.9以上: 完全な円環（フーリエ表現学習済み）
- 0.5-0.9: 部分的な構造
- 0.5以下: 構造なし

**なぜ円環になるのか:**
```
モデルが学習する表現: [cos(2πks/p), sin(2πks/p)]
これは単位円上の点: 角度 = 2πks/p

s=0 → 角度 0°
s=1 → 角度 約3.7° (= 360°/97)
s=2 → 角度 約7.4°
...
s=96 → 角度 約356°
```

---

### 2. ニューロン相関行列（右パネル 7×7グリッド）

```
     d0   d1   d2   d3   d4   d5   d6
d0  [自己] [・・] [・・] [・・] [・・] [・・] [・・]
d1  [・・] [自己] [・・] [・・] [・・] [・・] [・・]
d2  [・・] [・・] [自己] [・・] [・・] [・・] [・・]
...
```

**何を表しているか:**
- 7つの重要な次元（フーリエ相関が高い）を選択
- 各セル (i, j) は次元 i vs 次元 j の散布図
- 各点は異なる入力 (a, b) に対するニューロン出力

**解釈方法:**

| パターン | 見た目 | 意味 |
|----------|--------|------|
| 円形・楕円形 | ○ | cos/sin ペア（フーリエ基底）|
| 直線的 | / | 2つの次元が強く相関 |
| ランダム散布 | ・・・ | 独立した次元 |
| 複数のクラスタ | ●●● | 離散的な特徴 |

**Grokking前後の変化:**

```
【Grokking前】          【Grokking後】
・・・・・・              ○○○○○
・・・・・・     →       ○○○○○
・・・・・・              ○○○○○
（ランダム）            （円形パターン出現）
```

**色の意味:**
- Plasma カラーマップ（紫→黄）
- 入力のインデックスで色付け
- 同じ色の点が集まる→入力の構造を捉えている

---

### 3. 学習曲線（下パネル）

```
Acc%
100├─────────────────────●●●●●●●●●●●●●●●●
   │                   ╱
 50├──────────────────╱
   │    ●●●●●●●●●●●●●●
  0├●●●●
   └────────────────────────────────────
   0        500      1000     1500     2000  Epoch
        青: Train Acc    赤: Test Acc
        黄色縦線: 現在のエポック
```

**解釈方法:**

1. **青線（Train Accuracy）**: 訓練データへの適合度
   - 早期に100%到達 = 訓練データを記憶

2. **赤線（Test Accuracy）**: 汎化性能
   - 長く0%付近 = 汎化できていない
   - 急上昇 = Grokking発生

3. **黄色縦線**: アニメーションの現在位置

**典型的なパターン:**
```
Phase 1 (0-50):     両方とも低い（学習開始）
Phase 2 (50-200):   青↑100%, 赤→0%（過学習）
Phase 3 (200-250):  赤が急上昇（Grokking!）
Phase 4 (250+):     両方100%（汎化完了）
```

---

### 4. フーリエスペクトル（Fourier Analysis タブ）

```
Power
  │    ■
  │    ■
  │    ■  ■
  │    ■  ■     ■
  │────■──■─────■────────
  0    k1 k2    k3   ... Frequency k
```

**何を表しているか:**
- 埋め込み重みの離散フーリエ変換（DFT）
- 各周波数 k のパワー（振幅の2乗）

**解釈方法:**
- **支配的な周波数が少ない**: モデルが特定のフーリエ基底を学習
- **フラットなスペクトル**: 構造的な表現がない
- 通常 k=1 付近が支配的（基本周波数）

---

### 5. 埋め込み円環（Fourier Analysis タブ）

```
      ●2
   ●1    ●3
  ●0      ●4
   ●96   ●5
      ●95
```

**何を表しているか:**
- トークン埋め込み（0〜96）の2D射影
- 分散が最大の2次元を選択

**解釈方法:**
- **きれいな円**: トークンがフーリエ基底で埋め込まれている
- **隣接トークンが隣接**: 数値の順序関係を学習
- **Angle Correlation**: 角度と理論値の相関

**注意**: これは「単一トークン」の埋め込み。Epoch Sliderの円環は「和の表現」で異なる。

---

### 6. MLP出力行列（Model Output タブ）

```
     b →
   0 1 2 3 ...
a 0[0 1 2 3 ...]
↓ 1[1 2 3 4 ...]
  2[2 3 4 5 ...]
```

**何を表しているか:**
- 入力 (a, b) に対するモデルの予測値
- 正解は (a + b) mod p

**解釈方法:**
- **対角線パターン**: 正しい加算を学習
- **ランダム**: 未学習
- **Accuracy**: 正解率（100%が目標）

---

## トラブルシューティング

### 円環が出ない場合

1. **エポックが足りない**: 200以上まで進める
2. **weight_decay が小さい**: 1.0 推奨
3. **train_ratio が大きすぎる**: 0.3以下推奨

### Circle値が低い場合

- Grokking前は正常（0.1-0.3）
- Grokking後も低い場合はモデルが別の解法を学習している可能性

## 参考文献

1. Power, A., et al. (2022). "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"
2. Neel Nanda et al. (2023). "Progress measures for grokking via mechanistic interpretability"

## ファイル構成

```
test_NN/
├── model.py                 # Transformerモデル定義
├── train_grokking_demo.py   # 学習スクリプト
├── interactive_dashboard.py # 可視化ダッシュボード
├── analyze.py               # フーリエ解析
├── checkpoints_demo/        # 10epoch刻み
├── checkpoints_demo_2ep/    # 2epoch刻み
└── checkpoints_demo_5ep/    # 5epoch刻み
```
